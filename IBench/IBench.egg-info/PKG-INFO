Metadata-Version: 2.4
Name: IBench
Version: 0.1.0
Summary: Model-User Dialogue Evaluation Framework
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# IBench - å¯¹è¯æ¨¡å‹è¯„ä¼°æ¡†æ¶

IBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¯¹è¯æ¨¡å‹è´¨é‡çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒ**é»„é‡‘å†å²è¯„ä¼°**å’Œ**åŠ¨æ€äº¤äº’è¯„ä¼°**ä¸¤ç§æ¨¡å¼ã€‚

## ğŸ“Š è¯„ä¼°ä½“ç³»

IBench æä¾›ä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼Œæ»¡è¶³ä¸åŒçš„è¯„ä¼°éœ€æ±‚ï¼š

| æ¨¡å¼ | è¯„ä¼°å™¨ | è¾“å…¥ | ç”¨é€” |
|------|--------|------|------|
| **é»„é‡‘å†å²è¯„ä¼°** | `JsonContextEvaluator` | å®Œæ•´å¯¹è¯ï¼ˆæœ€åä¸€æ¡æ˜¯useræ¶ˆæ¯ï¼‰ | æµ‹è¯•æ¨¡å‹åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­çš„å›å¤èƒ½åŠ› |
| **åŠ¨æ€äº¤äº’è¯„ä¼°** | `DynamicInteractiveEvaluator` | å®Œæ•´å¯¹è¯ï¼ˆæ‰€æœ‰è½®æ¬¡ï¼‰ | è¯„ä¼°å®Œæ•´å¯¹è¯çš„è´¨é‡ |

> ğŸ“– è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ï¼š[è¯„ä¼°ä½“ç³»è¯´æ˜æ–‡æ¡£](docs/è¯„ä¼°ä½“ç³»è¯´æ˜.md)

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- **é»„é‡‘å†å²è¯„ä¼°**ï¼šç»™å®šå®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå¹¶è¯„ä¼°æœ€åä¸€æ¡å›å¤
- **åŠ¨æ€äº¤äº’è¯„ä¼°**ï¼šè¯„ä¼°å·²æœ‰çš„å®Œæ•´å¯¹è¯è®°å½•ï¼Œæ”¯æŒFIRST_Nå’ŒN_thè§„åˆ™
- **çµæ´»çš„è§„åˆ™ç³»ç»Ÿ**ï¼š
  - 5æ¡å•è½®è§„åˆ™ï¼ˆé£æ ¼ã€æé—®ã€åŒ»ç–—è¾¹ç•Œï¼‰
  - 11æ¡é˜¶æ®µè§„åˆ™ï¼ˆå’¨è¯¢ã€è½¬åŒ–ã€èŒƒå›´æ§åˆ¶ç­‰ï¼‰
  - æ”¯æŒFIRST_Nï¼ˆå‰Nè½®è‡³å°‘è§¦å‘ä¸€æ¬¡ï¼‰å’ŒN_thï¼ˆç¬¬Nè½®å¿…é¡»è§¦å‘ï¼‰è§„åˆ™ç±»å‹
- **æ··åˆè¯„ä¼°æ–¹å¼**ï¼šæ”¯æŒåŸºäºè§„åˆ™çš„è‡ªåŠ¨è¯„ä¼°å’ŒåŸºäºLLMçš„æ™ºèƒ½è¯„ä¼°
- **å‚æ•°è‡ªåŠ¨æå–**ï¼šä½¿ç”¨LLMè‡ªåŠ¨æå–kwargså¹¶å†™å…¥è¯„ä¼°ç»“æœ
- **JSONé…ç½®æ–¹å¼**ï¼šæ‰€æœ‰è¯„ä¼°é€šè¿‡JSONæ–‡ä»¶é…ç½®ï¼Œæ˜“äºç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é»„é‡‘å†å²è¯„ä¼°

æµ‹è¯•æ¨¡å‹åœ¨ç‰¹å®šå¯¹è¯ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆå›å¤çš„èƒ½åŠ›ï¼š

```python
from IBench.pipeline.json_context_evaluator import JsonContextEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = JsonContextEvaluator(api_key="your-api-key")

# ä»JSONæ–‡ä»¶è¯„ä¼°
result = evaluator.evaluate_from_json(
    input_json_path="examples/golden_history_input_example.json",
    output_json_path="data/output/result.json"
)

print(f"ç”Ÿæˆçš„å›å¤: {result['generated_response']}")
print(f"æ€»å¾—åˆ†: {sum(e['score'] for e in result['evaluations'])}")
```

**è¾“å…¥JSONç¤ºä¾‹ï¼š**
```json
{
  "key": "001",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "å­©å­å¤ªçŸ®"},
    {"role": "assistant", "content": "è¯·é—®æ˜¯ä¸ºè°å’¨è¯¢ï¼Ÿ"},
    {"role": "user", "content": "5å²"}
  ],
  "rule_list": [
    "single_turn:sty:gratitude",
    {"rule": "multi_turn:N_th:consult_subject", "N": 1}
  ]
}
```

### 2. åŠ¨æ€äº¤äº’è¯„ä¼°

è¯„ä¼°å®Œæ•´å¯¹è¯è®°å½•çš„è´¨é‡ï¼š

```python
from IBench.pipeline.dynamic_interactive_eval import DynamicInteractiveEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = DynamicInteractiveEvaluator(api_key="your-api-key")

# ä»JSONæ–‡ä»¶è¯„ä¼°
result = evaluator.evaluate_from_json(
    input_json_path="examples/dynamic_interactive_input_example.json",
    output_json_path="data/output/result.json"
)

print(f"æ€»å¾—åˆ†: {result['summary']['total_score']}")
print(f"æ€»è½®æ¬¡: {result['summary']['total_turns']}")
```

### 3. é…ç½®ç¯å¢ƒå˜é‡

```bash
export DASHSCOPE_API_KEY="your-api-key"
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€æµ‹è¯•ï¼ˆæ— éœ€ä¾èµ–ï¼‰

```bash
# æµ‹è¯• IBench æ ¸å¿ƒåŠŸèƒ½
python IBench/scripts/quick_test.py
```

### 2. å®‰è£…ä¾èµ–

```bash
# å®Œæ•´åŠŸèƒ½
pip install torch transformers openai bitsandbytes accelerate
```

### 3. æµ‹è¯•æ¨¡å‹åŠ è½½

```bash
# åœ¨æœåŠ¡å™¨ä¸Šæµ‹è¯•
python IBench/scripts/test_model_loading.py
```

### 4. é…ç½®ç¯å¢ƒå˜é‡

```bash
export DASHSCOPE_API_KEY="your-api-key"
```

### ç¯å¢ƒå†å²è¯„ä¼°ç¤ºä¾‹

```python
from IBench import ContextEvaluator, Message, EvaluationMode

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = ContextEvaluator(
    local_model_path="./models/qwen3-8b",
    api_key="your-api-key"
)

# å®šä¹‰å¯¹è¯å†å²
conversation_history = [
    Message(role="user", content="æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ", turn_id=1),
    Message(role="user", content="æ¯å¤©æ™šä¸Šéƒ½ç¡ä¸ç€ï¼Œå¾ˆç„¦è™‘", turn_id=2)
]

# æ‰§è¡Œè¯„ä¼°
result = evaluator.evaluate(conversation_history)

# æŸ¥çœ‹ç»“æœ
print(f"æœ€ç»ˆå¾—åˆ†: {result.final_score}")
print(f"æ€»è½®æ¬¡: {result.total_turns}")
```

### åŠ¨æ€äº¤äº’è¯„ä¼°ç¤ºä¾‹

```python
from IBench import InteractiveEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = InteractiveEvaluator(
    local_model_path="./models/qwen3-8b",
    api_key="your-api-key"
)

# å®šä¹‰åˆå§‹ç”¨æˆ·è¾“å…¥
initial_prompt = "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ "

# æ‰§è¡Œè¯„ä¼°
result = evaluator.evaluate(initial_prompt, max_turns=5)

# æŸ¥çœ‹ç»“æœ
print(f"æœ€ç»ˆå¾—åˆ†: {result.final_score}")
```

## ğŸ“‹ è§„åˆ™ä½“ç³»

### è§„åˆ™å‘½åæ ¼å¼

#### å•è½®è§„åˆ™
```
single_turn:{category}:{rule_name}
```
- `sty`: é£æ ¼å£ç™–ï¼ˆå®¢å¥—/è§£é‡Šæ€§åºŸè¯ï¼‰
- `ask`: æé—®ä¸æ¾„æ¸…ï¼ˆå¯¹è±¡/å¼•å¯¼/ä¸€è½®å¤šé—®ï¼‰
- `med`: åŒ»ç–—è¾¹ç•Œï¼ˆç—‡çŠ¶å¼ºå¼•å¯¼/è¯Šæ–­å/å°±è¯Šå²/æ£€æŸ¥é‚€çº¦ï¼‰

ç¤ºä¾‹ï¼š
- `single_turn:sty:gratitude` - ä½¿ç”¨æ„Ÿè°¢ç”¨è¯­
- `single_turn:sty:explain_filler` - ä½¿ç”¨è§£é‡Šæ€§å¥—è¯
- `single_turn:med:forced_symptom` - ä½¿ç”¨å®½æ³›é—®è¯Šè¯­å¥

#### é˜¶æ®µè§„åˆ™
```
multi_turn:{FIRST_N|N_th}:{rule_name}
```

- **FIRST_N**: åœ¨å‰Nè½®ä¸­**è‡³å°‘è§¦å‘ä¸€æ¬¡**
- **N_th**: åœ¨ç¬¬Nè½®**å¿…é¡»è§¦å‘**

ç¤ºä¾‹ï¼š
- `multi_turn:FIRST_N:consult_subject` + `N=1` â†’ åœ¨ç¬¬1è½®è¯¢é—®å’¨è¯¢å¯¹è±¡
- `multi_turn:N_th:gender` + `N=4` â†’ åœ¨ç¬¬4è½®è¯¢é—®æ€§åˆ«

### è§„åˆ™åˆ—è¡¨

#### å•è½®è§„åˆ™ï¼ˆ5æ¡ï¼‰

| Tag | çº¦æŸ | å¾—åˆ† | ç±»å‹ |
|-----|------|------|------|
| `single_turn:sty:gratitude` | ä½¿ç”¨æ„Ÿè°¢ç”¨è¯­ | -1 | LLM |
| `single_turn:sty:explain_filler` | ä½¿ç”¨è§£é‡Šæ€§å¥—è¯ | -1 | LLM |
| `single_turn:med:forced_symptom` | ä½¿ç”¨å®½æ³›é—®è¯Šè¯­å¥ | -1 | LLM |
| `single_turn:ask:multi_question` | ä¸€æ¡æ¶ˆæ¯é—®å¤šä¸ªé—®é¢˜ | -1 | Rule |
| `single_turn:med:diagnosis_name` | ç›´æ¥ç»™å‡ºç–¾ç—…åç§° | -1 | LLM |

#### é˜¶æ®µè§„åˆ™ï¼ˆ11æ¡ï¼‰

| Tag | çº¦æŸ | å‰ç½®æ¡ä»¶ | å¾—åˆ† | ç±»å‹ |
|-----|------|----------|------|------|
| `multi_turn:*:consult_subject` | è¯¢é—®ç”¨æˆ·ä¸ºè°å’¨è¯¢ | - | +1 | LLM |
| `multi_turn:*:visit_history` | æåŠç”¨æˆ·å°±è¯Šå² | - | -1 | LLM |
| `multi_turn:*:test_invite` | æå‡ºæ£€æŸ¥é‚€çº¦ | ç”¨æˆ·æ²¡ææ£€æŸ¥ | -1 | LLM |
| `multi_turn:*:gender` | è¯¢é—®æ€§åˆ« | - | +1 | Rule |
| `multi_turn:*:medication_phone` | ä»¥ç”¨è¯ä¸ºç†ç”±å¥—å–ç”µè¯ | ç”¨æˆ·æåŠç”¨è¯å² | +1 | LLM |
| `multi_turn:*:complication_phone` | ä»¥å¹¶å‘ç—‡ä¸ºç†ç”±å¥—å–ç”µè¯ | ç”¨æˆ·å¹´é¾„>=60å² | +1 | LLM |
| `multi_turn:*:expert_phone` | ä»¥ä¸“å®¶è§£è¯»ä¸ºç†ç”±å¥—å–ç”µè¯ | ç”¨æˆ·æœªå°±è¯Š | +1 | LLM |
| `multi_turn:*:primary_only` | ä»…å›´ç»•ä¸»è¦ç—…ç—‡ç»™å‡ºå›å¤ | ç”¨æˆ·æåŠå¤šç§ç–¾ç—… | +1 | LLM |
| `multi_turn:*:prompt_question` | ç»™å‡ºå¼•å¯¼ | ç”¨æˆ·æœªç»™æ˜ç¡®é—®é¢˜ | +1 | LLM |
| `multi_turn:*:report_phone` | ç»“åˆæŠ¥å‘Šå¥—å–ç”µè¯ | ç”¨æˆ·å·²å°±è¯Šä¸”æåŠæ£€æŸ¥æŠ¥å‘Š | +1 | LLM |
| `multi_turn:*:advice_phone` | ä»¥ç”¨è¯å»ºè®®ä¸ºç†ç”±å¥—å–ç”µè¯ | ç”¨æˆ·æ­£åœ¨æœè¯å¹¶æ±‚å»ºè®® | +1 | LLM |

> ğŸ“– å®Œæ•´è§„åˆ™è¯´æ˜è¯·æŸ¥çœ‹ï¼š[è¯„ä¼°Bench.md](docs/è¯„ä¼°Bench.md)

## ğŸ“ ç›®å½•ç»“æ„

```
IBench/
â”œâ”€â”€ config.py                           # é…ç½®ç®¡ç†
â”œâ”€â”€ models/                             # æ¨¡å‹åŠ è½½å™¨
â”‚   â”œâ”€â”€ local_model.py                 # æœ¬åœ°æ¨¡å‹ï¼ˆHuggingFaceï¼‰
â”‚   â””â”€â”€ api_model.py                   # APIæ¨¡å‹ï¼ˆDashscopeï¼‰
â”œâ”€â”€ rules/                              # è§„åˆ™å®šä¹‰å’Œè¯„ä¼°
â”‚   â”œâ”€â”€ single_rules.py                # å•è½®è§„åˆ™ï¼ˆ5æ¡ï¼‰
â”‚   â”œâ”€â”€ stage_rules.py                 # é˜¶æ®µè§„åˆ™ï¼ˆ11æ¡ï¼‰
â”‚   â”œâ”€â”€ dynamic_rule_registry.py       # åŠ¨æ€è§„åˆ™æ³¨å†Œå™¨
â”‚   â”œâ”€â”€ rule_mappings.py               # è§„åˆ™æ˜ å°„é…ç½®
â”‚   â””â”€â”€ kwargs_extractor.py            # å‚æ•°æå–å™¨
â”œâ”€â”€ conversation/                       # å¯¹è¯ç®¡ç†
â”‚   â”œâ”€â”€ conversation.py                # å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â””â”€â”€ user_simulator.py              # ç”¨æˆ·æ¨¡æ‹Ÿå™¨
â”œâ”€â”€ evaluator/                          # è¯„ä¼°å¼•æ“
â”‚   â”œâ”€â”€ rule_evaluator.py              # è§„åˆ™è¯„ä¼°å™¨
â”‚   â””â”€â”€ batch_evaluator.py             # æ‰¹é‡è¯„ä¼°å™¨
â”œâ”€â”€ pipeline/                           # è¯„ä¼°æµç¨‹
â”‚   â”œâ”€â”€ json_context_evaluator.py      # é»„é‡‘å†å²è¯„ä¼°å™¨ â­
â”‚   â””â”€â”€ dynamic_interactive_eval.py    # åŠ¨æ€äº¤äº’è¯„ä¼°å™¨ â­
â”œâ”€â”€ utils/                              # å·¥å…·æ¨¡å—
â”‚   â””â”€â”€ common.py                      # é€šç”¨æ•°æ®ç»“æ„
â”œâ”€â”€ examples/                           # ç¤ºä¾‹å’Œæµ‹è¯•
â”‚   â”œâ”€â”€ golden_history_input_example.json      # é»„é‡‘å†å²è¯„ä¼°è¾“å…¥ç¤ºä¾‹
â”‚   â”œâ”€â”€ golden_history_output_example.json     # é»„é‡‘å†å²è¯„ä¼°è¾“å‡ºç¤ºä¾‹
â”‚   â”œâ”€â”€ dynamic_interactive_input_example.json  # åŠ¨æ€äº¤äº’è¯„ä¼°è¾“å…¥ç¤ºä¾‹
â”‚   â””â”€â”€ dynamic_interactive_output_example.json # åŠ¨æ€äº¤äº’è¯„ä¼°è¾“å‡ºç¤ºä¾‹
â”œâ”€â”€ docs/                               # æ–‡æ¡£
â”‚   â”œâ”€â”€ è¯„ä¼°Bench.md                   # è¯„ä¼°è§„åˆ™è¯¦ç»†è¯´æ˜
â”‚   â””â”€â”€ è¯„ä¼°ä½“ç³»è¯´æ˜.md                # è¯„ä¼°ä½“ç³»å®Œæ•´è¯´æ˜
â””â”€â”€ deprecated/                         # å·²åºŸå¼ƒçš„è¯„ä¼°å™¨
    â”œâ”€â”€ context_eval.py                # æ—§çš„ç¯å¢ƒå†å²è¯„ä¼°å™¨
    â”œâ”€â”€ interactive_eval.py            # æ—§çš„åŠ¨æ€äº¤äº’è¯„ä¼°å™¨
    â””â”€â”€ README.md                      # åºŸå¼ƒè¯´æ˜å’Œè¿ç§»æŒ‡å—
```

â­ è¡¨ç¤ºä¸»è¦çš„è¯„ä¼°å™¨å…¥å£

## ğŸ“š æ›´å¤šèµ„æº

### æ–‡æ¡£
- **[è¯„ä¼°ä½“ç³»è¯´æ˜](docs/è¯„ä¼°ä½“ç³»è¯´æ˜.md)** - ä¸¤ç§è¯„ä¼°æ¨¡å¼çš„è¯¦ç»†å¯¹æ¯”å’Œä½¿ç”¨æŒ‡å—
- **[è¯„ä¼°Bench](docs/è¯„ä¼°Bench.md)** - å®Œæ•´çš„è§„åˆ™å®šä¹‰å’Œè¯´æ˜
- **[deprecated/README.md](deprecated/README.md)** - å·²åºŸå¼ƒè¯„ä¼°å™¨çš„è¿ç§»æŒ‡å—

### ç¤ºä¾‹æ–‡ä»¶
- `examples/golden_history_input_example.json` - é»„é‡‘å†å²è¯„ä¼°è¾“å…¥ç¤ºä¾‹
- `examples/golden_history_output_example.json` - é»„é‡‘å†å²è¯„ä¼°è¾“å‡ºç¤ºä¾‹
- `examples/dynamic_interactive_input_example.json` - åŠ¨æ€äº¤äº’è¯„ä¼°è¾“å…¥ç¤ºä¾‹
- `examples/dynamic_interactive_output_example.json` - åŠ¨æ€äº¤äº’è¯„ä¼°è¾“å‡ºç¤ºä¾‹

## âš ï¸ é‡è¦å˜æ›´

### å·²åºŸå¼ƒçš„è¯„ä¼°æ–¹å¼

ä»¥ä¸‹è¯„ä¼°å™¨å·²è¢«JSONæ–¹å¼æ›¿ä»£ï¼Œä¸å†æ¨èä½¿ç”¨ï¼š
- âŒ `ContextEvaluator` (ä»£ç æ–¹å¼) â†’ âœ… `JsonContextEvaluator` (JSONæ–¹å¼)
- âŒ `InteractiveEvaluator` (ä»£ç æ–¹å¼) â†’ âœ… `DynamicInteractiveEvaluator` (JSONæ–¹å¼)

è¯¦è§ï¼š[deprecated/README.md](deprecated/README.md)

### æ–°è§„åˆ™æ ¼å¼

è§„åˆ™å‘½åå·²æ›´æ–°ä¸ºç»Ÿä¸€æ ¼å¼ï¼š
- å•è½®è§„åˆ™ï¼š`single_turn:{category}:{rule_name}`
- é˜¶æ®µè§„åˆ™ï¼š`multi_turn:{FIRST_N|N_th}:{rule_name}`

æ—§çš„è§„åˆ™æ˜ å°„ä»ç„¶æ”¯æŒï¼Œä½†å»ºè®®ä½¿ç”¨æ–°æ ¼å¼ã€‚

## ğŸ”§ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```python
from IBench.config import ModelConfig

model_config = ModelConfig(
    local_model_path="./models/qwen3-8b",
    api_key="your-api-key",
    user_model_name="qwen-plus",      # ç”¨æˆ·æ¨¡æ‹Ÿå™¨æ¨¡å‹
    judge_model_name="qwen-max",      # è¯„ä¼°judgeæ¨¡å‹
    temperature=0.0,
    max_new_tokens=512
)
```

### è¯„ä¼°é…ç½®

```python
from IBench.config import EvaluationConfig

eval_config = EvaluationConfig(
    output_dir="./data/output",
    batch_size=8,
    max_conversation_turns=10,
    single_rule_turns={
        1: [1, 2, 3, 4, 5, 6],
        2: [1, 2, 3, 4, 5, 6]
    },
    stage_rule_turns={
        1: [1],
        3: [2, 3],
        4: [4]
    }
)
```

## è¾“å‡ºæ ¼å¼

### è¯„ä¼°ç»“æœç¤ºä¾‹

```json
{
  "conversation_id": "context_conv_2",
  "mode": "context",
  "total_turns": 2,
  "final_score": -2,
  "metadata": {},
  "turn_evaluations": [
    {
      "turn_id": 1,
      "response": "æ‚¨çš„å¤±çœ é—®é¢˜éœ€è¦é‡è§†...",
      "single_rules": [
        {
          "rule_id": 1,
          "rule_type": "LLM",
          "rule_description": "ä½¿ç”¨æƒ…æ„Ÿå®‰æ…°ç”¨è¯­",
          "triggered": true,
          "score": -1,
          "reason": "VIOLATED"
        }
      ],
      "stage_rules": [],
      "total_score": -1
    }
  ]
}
```

## è®¸å¯è¯

MIT License
