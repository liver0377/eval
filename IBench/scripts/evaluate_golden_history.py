"""
æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²è¯„æµ‹æ•°æ®é›†ï¼ˆJSONLæ ¼å¼ï¼‰
ç”¨äºè¯„ä¼° data/dataset/golden_history_input.jsonl ä¸­çš„80æ¡æµ‹è¯•ç”¨ä¾‹
"""

import json
import os
import sys
import uuid
import threading
from pathlib import Path
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from IBench.pipeline.json_context_evaluator import JsonContextEvaluator
from IBench.config import Config


def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½JSONLæ–‡ä»¶
    
    Args:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
    
    Returns:
        JSONå¯¹è±¡åˆ—è¡¨
    """
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def evaluate_single_entry(
    entry: Dict[str, Any],
    index: int,
    total: int,
    output_dir: str,
    evaluator: 'JsonContextEvaluator',
    print_lock: threading.Lock
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    è¯„ä¼°å•ä¸ªæ¡ç›®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        entry: æ•°æ®æ¡ç›®
        index: æ¡ç›®ç´¢å¼•
        total: æ€»æ¡ç›®æ•°
        output_dir: è¾“å‡ºç›®å½•
        evaluator: è¯„ä¼°å™¨
        print_lock: æ‰“å°é”
    
    Returns:
        (è¯„ä¼°ç»“æœ, è¯¦æƒ…ä¿¡æ¯) æˆ– (None, é”™è¯¯ä¿¡æ¯)
    """
    key = entry.get('key', f'entry_{index}')
    unique_id = str(uuid.uuid4())[:8]
    temp_input_path = os.path.join(output_dir, f"temp_input_{key}_{unique_id}.json")
    
    try:
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°
        with print_lock:
            print(f"[{index}/{total}] è¯„ä¼°æ¡ç›® {key}")
        
        # ä¿å­˜ä¸´æ—¶è¾“å…¥æ–‡ä»¶
        with open(temp_input_path, 'w', encoding='utf-8') as f:
            json.dump(entry, f, ensure_ascii=False, indent=2)
        
        # è¯„ä¼°
        result = evaluator.evaluate_from_json(temp_input_path, None)
        
        # æå–è¯„ä¼°ç»“æœ
        evaluations = result.get('evaluations', [])
        total_score = sum(e.get('score', 0) for e in evaluations)
        triggered_rules = [e['rule'] for e in evaluations if e.get('triggered', False)]
        
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°ç»“æœ
        with print_lock:
            print(f"  âœ“ {key} å¾—åˆ†: {total_score}, è§¦å‘è§„åˆ™: {len(triggered_rules)}/{len(evaluations)}")
        
        # è®°å½•è¯¦æƒ…
        detail = {
            "key": key,
            "score": total_score,
            "triggered_count": len(triggered_rules),
            "total_rules": len(evaluations),
            "triggered_rules": triggered_rules
        }
        
        return result, detail
        
    except Exception as e:
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°é”™è¯¯
        with print_lock:
            print(f"  âœ— {key} è¯„ä¼°å¤±è´¥: {e}")
        
        error_detail = {
            "key": key,
            "error": str(e)
        }
        return None, error_detail
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(temp_input_path)
        except:
            pass


def evaluate_golden_history_jsonl(
    jsonl_path: str,
    output_dir: str,
    api_key: str = None
) -> Dict[str, Any]:
    """
    æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²JSONLæ•°æ®é›†
    
    Args:
        jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {jsonl_path}")
    entries = load_jsonl(jsonl_path)
    print(f"âœ“ æ‰¾åˆ° {len(entries)} ä¸ªæ¡ç›®\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    config = Config()
    if api_key:
        config.model.api_key = api_key
    evaluator = JsonContextEvaluator(config=config)
    print("âœ“ è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ\n")
    
    # æ‰¹é‡è¯„ä¼°
    output_jsonl_path = os.path.join(output_dir, "golden_history_output.jsonl")
    results = []
    summary = {
        "total_entries": len(entries),
        "evaluated_entries": 0,
        "failed_entries": 0,
        "total_score": 0,
        "total_rules_checked": 0,
        "total_rules_triggered": 0,
        "entry_details": []
    }
    
    print("=" * 60)
    print("å¼€å§‹æ‰¹é‡è¯„ä¼°ï¼ˆå¹¶å‘æ¨¡å¼ï¼Œ5çº¿ç¨‹ï¼‰")
    print("=" * 60 + "\n")
    
    # åˆ›å»ºæ‰“å°é”
    print_lock = threading.Lock()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è¯„ä¼°
    with ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_entry = {
            executor.submit(evaluate_single_entry, entry, i, len(entries), output_dir, evaluator, print_lock): entry
            for i, entry in enumerate(entries, 1)
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_entry):
            result, detail = future.result()
            
            if result is not None:
                # æˆåŠŸè¯„ä¼°
                results.append(result)
                summary['evaluated_entries'] += 1
                summary['total_score'] += detail['score']
                summary['total_rules_checked'] += detail['total_rules']
                summary['total_rules_triggered'] += detail['triggered_count']
                summary['entry_details'].append(detail)
            else:
                # è¯„ä¼°å¤±è´¥
                summary['failed_entries'] += 1
                summary['entry_details'].append(detail)
    
    # ç»Ÿä¸€å†™å…¥JSONLæ–‡ä»¶
    print("\n" + "=" * 60)
    print("å†™å…¥è¯„ä¼°ç»“æœ...")
    print("=" * 60)
    with open(output_jsonl_path, 'w', encoding='utf-8') as output_f:
        for result in results:
            output_line = json.dumps(result, ensure_ascii=False)
            output_f.write(output_line + '\n')
    print(f"âœ“ å·²å†™å…¥ {len(results)} æ¡ç»“æœåˆ° {output_jsonl_path}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("=" * 60)
    print("è¯„ä¼°æ±‡æ€»")
    print("=" * 60)
    
    avg_score = summary['total_score'] / max(summary['evaluated_entries'], 1)
    avg_triggered = summary['total_rules_triggered'] / max(summary['total_rules_checked'], 1) * 100
    
    summary['average_score'] = round(avg_score, 2)
    summary['average_triggered_percent'] = round(avg_triggered, 2)
    
    print(f"æ€»æ¡ç›®æ•°: {summary['total_entries']}")
    print(f"æˆåŠŸè¯„ä¼°: {summary['evaluated_entries']}")
    print(f"å¤±è´¥æ•°é‡: {summary['failed_entries']}")
    print(f"æ€»å¾—åˆ†: {summary['total_score']}")
    print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")
    print(f"è§„åˆ™è§¦å‘ç‡: {avg_triggered:.1f}%")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = os.path.join(output_dir, "evaluation_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_jsonl_path}")
    print(f"âœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
    
    return summary


def main():
    """ä¸»å‡½æ•°"""
    # è·¯å¾„é…ç½®
    jsonl_path = "data/dataset/golden_history_input.jsonl"
    output_dir = "data/output/golden_history_eval"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(jsonl_path):
        print(f"âœ— é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        print(f"  å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        return 1
    
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    api_key = os.getenv("DASHSCOPE_API_KEY")
    
    if not api_key:
        print("âš  è­¦å‘Š: DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("  è¯„ä¼°å¯èƒ½æ— æ³•æ­£å¸¸è¿›è¡Œï¼ˆå¦‚éœ€è¦LLM judgeï¼‰")
    
    # æ‰¹é‡è¯„ä¼°
    try:
        summary = evaluate_golden_history_jsonl(jsonl_path, output_dir, api_key)
        print("\nâœ“ æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        return 0
    except Exception as e:
        print(f"\nâœ— æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
