"""
æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²è¯„æµ‹æ•°æ®é›†ï¼ˆJSONLæ ¼å¼ï¼‰
ç”¨äºè¯„ä¼° data/dataset/golden_history_input.jsonl ä¸­çš„80æ¡æµ‹è¯•ç”¨ä¾‹
"""

import json
import os
import sys
import uuid
import threading
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from IBench.pipeline.json_context_evaluator import JsonContextEvaluator
from IBench.models.model_configs import Config, list_available_models


def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½JSONLæ–‡ä»¶
    
    Args:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
    
    Returns:
        JSONå¯¹è±¡åˆ—è¡¨
    """
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def evaluate_single_entry(
    entry: Dict[str, Any],
    index: int,
    total: int,
    output_dir: str,
    evaluator: 'JsonContextEvaluator',
    print_lock: threading.Lock
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    è¯„ä¼°å•ä¸ªæ¡ç›®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        entry: æ•°æ®æ¡ç›®
        index: æ¡ç›®ç´¢å¼•
        total: æ€»æ¡ç›®æ•°
        output_dir: è¾“å‡ºç›®å½•
        evaluator: è¯„ä¼°å™¨
        print_lock: æ‰“å°é”
    
    Returns:
        (è¯„ä¼°ç»“æœ, è¯¦æƒ…ä¿¡æ¯) æˆ– (None, é”™è¯¯ä¿¡æ¯)
    """
    key = entry.get('key', f'entry_{index}')
    unique_id = str(uuid.uuid4())[:8]
    temp_input_path = os.path.join(output_dir, f"temp_input_{key}_{unique_id}.json")
    
    try:
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°
        with print_lock:
            print(f"[{index}/{total}] è¯„ä¼°æ¡ç›® {key}")
        
        # ä¿å­˜ä¸´æ—¶è¾“å…¥æ–‡ä»¶
        with open(temp_input_path, 'w', encoding='utf-8') as f:
            json.dump(entry, f, ensure_ascii=False, indent=2)
        
        # è¯„ä¼°
        result = evaluator.evaluate_from_json(temp_input_path, None)
        
        # æå–è¯„ä¼°ç»“æœ
        evaluations = result.get('evaluations', [])
        total_score = sum(e.get('score', 0) for e in evaluations)
        triggered_rules = [e['rule'] for e in evaluations if e.get('triggered', False)]
        
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°ç»“æœ
        with print_lock:
            print(f"  âœ“ {key} å¾—åˆ†: {total_score}, è§¦å‘è§„åˆ™: {len(triggered_rules)}/{len(evaluations)}")
        
        # è®°å½•è¯¦æƒ…
        detail = {
            "key": key,
            "score": total_score,
            "triggered_count": len(triggered_rules),
            "total_rules": len(evaluations),
            "triggered_rules": triggered_rules
        }
        
        return result, detail
        
    except Exception as e:
        # çº¿ç¨‹å®‰å…¨çš„æ‰“å°é”™è¯¯
        with print_lock:
            print(f"  âœ— {key} è¯„ä¼°å¤±è´¥: {e}")
        
        error_detail = {
            "key": key,
            "error": str(e)
        }
        return None, error_detail
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(temp_input_path)
        except:
            pass


def evaluate_golden_history_jsonl(
    jsonl_path: str,
    output_dir: str,
    model_name: str = "Qwen3-8B",
    api_key: Optional[str] = None,
    workers: int = 5
) -> Dict[str, Any]:
    """
    æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²JSONLæ•°æ®é›†

    Args:
        jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        model_name: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        workers: å¹¶å‘çº¿ç¨‹æ•°

    Returns:
        æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {jsonl_path}")
    entries = load_jsonl(jsonl_path)
    print(f"âœ“ æ‰¾åˆ° {len(entries)} ä¸ªæ¡ç›®\n")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºè¯„ä¼°å™¨
    print(f"ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆæ¨¡å‹: {model_name}ï¼‰...")
    config = Config(model_name=model_name)
    if api_key:
        config.model.api_key = api_key
    evaluator = JsonContextEvaluator(config=config)
    print("âœ“ è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ\n")
    
    # æ‰¹é‡è¯„ä¼°
    output_jsonl_path = os.path.join(output_dir, "golden_history_output.jsonl")
    results = []
    summary = {
        "total_entries": len(entries),
        "evaluated_entries": 0,
        "failed_entries": 0,
        "total_score": 0,
        "total_rules_checked": 0,
        "total_rules_triggered": 0,
        "entry_details": []
    }
    
    print("=" * 60)
    print(f"å¼€å§‹æ‰¹é‡è¯„ä¼°ï¼ˆå¹¶å‘æ¨¡å¼ï¼Œ{workers}çº¿ç¨‹ï¼‰")
    print("=" * 60 + "\n")
    
    # åˆ›å»ºæ‰“å°é”
    print_lock = threading.Lock()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è¯„ä¼°
    with ThreadPoolExecutor(max_workers=workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_entry = {
            executor.submit(evaluate_single_entry, entry, i, len(entries), output_dir, evaluator, print_lock): entry
            for i, entry in enumerate(entries, 1)
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_entry):
            result, detail = future.result()
            
            if result is not None:
                # æˆåŠŸè¯„ä¼°
                results.append(result)
                summary['evaluated_entries'] += 1
                summary['total_score'] += detail['score']
                summary['total_rules_checked'] += detail['total_rules']
                summary['total_rules_triggered'] += detail['triggered_count']
                summary['entry_details'].append(detail)
            else:
                # è¯„ä¼°å¤±è´¥
                summary['failed_entries'] += 1
                summary['entry_details'].append(detail)
    
    # ç»Ÿä¸€å†™å…¥JSONLæ–‡ä»¶
    print("\n" + "=" * 60)
    print("å†™å…¥è¯„ä¼°ç»“æœ...")
    print("=" * 60)
    with open(output_jsonl_path, 'w', encoding='utf-8') as output_f:
        for result in results:
            output_line = json.dumps(result, ensure_ascii=False)
            output_f.write(output_line + '\n')
    print(f"âœ“ å·²å†™å…¥ {len(results)} æ¡ç»“æœåˆ° {output_jsonl_path}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("=" * 60)
    print("è¯„ä¼°æ±‡æ€»")
    print("=" * 60)
    
    avg_score = summary['total_score'] / max(summary['evaluated_entries'], 1)
    avg_triggered = summary['total_rules_triggered'] / max(summary['total_rules_checked'], 1) * 100
    
    summary['average_score'] = round(avg_score, 2)
    summary['average_triggered_percent'] = round(avg_triggered, 2)
    
    print(f"æ€»æ¡ç›®æ•°: {summary['total_entries']}")
    print(f"æˆåŠŸè¯„ä¼°: {summary['evaluated_entries']}")
    print(f"å¤±è´¥æ•°é‡: {summary['failed_entries']}")
    print(f"æ€»å¾—åˆ†: {summary['total_score']}")
    print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")
    print(f"è§„åˆ™è§¦å‘ç‡: {avg_triggered:.1f}%")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = os.path.join(output_dir, "evaluation_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_jsonl_path}")
    print(f"âœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
    
    return summary


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²è¯„æµ‹æ•°æ®é›†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆQwen3-8Bï¼‰
  python scripts/evaluate_golden_history.py

  # æŒ‡å®šæ¨¡å‹
  python scripts/evaluate_golden_history.py --model qwen3_full_sft

  # å®Œæ•´å‚æ•°
  python scripts/evaluate_golden_history.py --model llama_factory_psy1.32.1_lora_qwen2_7b_dpo --api-key your-key --workers 3

å¯ç”¨æ¨¡å‹:
  Qwen3-8B                              - åŸºç¡€ Qwen3-8B æ¨¡å‹
  qwen3_full_sft                        - Qwen3-8B SFT å¾®è°ƒç‰ˆæœ¬
  llama_factory_psy1.32.1_lora_qwen2_7b_dpo  - Qwen2-7B + LoRA + DPO
        """
    )

    parser.add_argument(
        '--model', '-m',
        type=str,
        default='Qwen3-8B',
        choices=['Qwen3-8B', 'qwen3_full_sft', 'llama_factory_psy1.32.1_lora_qwen2_7b_dpo'],
        help='æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼šQwen3-8Bï¼‰'
    )

    parser.add_argument(
        '--api-key', '-k',
        type=str,
        default=None,
        help='APIå¯†é’¥ï¼ˆé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡DASHSCOPE_API_KEYï¼‰'
    )

    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='data/output/golden_history_eval',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šdata/output/golden_history_evalï¼‰'
    )

    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=5,
        help='å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰'
    )

    parser.add_argument(
        '--list-models',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹å¹¶é€€å‡º'
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # å¤„ç† --list-models
    if args.list_models:
        print("=" * 60)
        print("å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š")
        print("=" * 60)
        for name, description in list_available_models().items():
            print(f"  {name:50s} - {description}")
        print("=" * 60)
        return 0

    # è·¯å¾„é…ç½®
    script_dir = Path(__file__).resolve().parent.parent
    jsonl_path = script_dir / "data/dataset/golden_history_input.jsonl"
    output_dir = script_dir / args.output_dir

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(jsonl_path):
        print(f"âœ— é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        print(f"  å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        return 1

    # API key ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
    api_key = args.api_key or os.getenv("DASHSCOPE_API_KEY")

    if not api_key:
        print("âš  è­¦å‘Š: DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ä¸”æœªé€šè¿‡ --api-key æä¾›")
        print("  è¯„ä¼°å¯èƒ½æ— æ³•æ­£å¸¸è¿›è¡Œï¼ˆå¦‚éœ€è¦LLM judgeï¼‰")

    # æ‰¹é‡è¯„ä¼°
    try:
        print(f"\né…ç½®ä¿¡æ¯:")
        print(f"  æ¨¡å‹: {args.model}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  å¹¶å‘çº¿ç¨‹: {args.workers}")
        print(f"  API Key: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
        print()

        summary = evaluate_golden_history_jsonl(
            jsonl_path=str(jsonl_path),
            output_dir=str(output_dir),
            model_name=args.model,
            api_key=api_key or None,
            workers=args.workers
        )
        print("\nâœ“ æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        return 0
    except Exception as e:
        print(f"\nâœ— æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
