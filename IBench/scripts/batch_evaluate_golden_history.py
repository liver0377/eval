"""
æ‰¹é‡è¯„ä¼°é»„é‡‘å†å²è¯„æµ‹æ•°æ®é›†
ä½¿ç”¨JsonContextEvaluatoræ‰¹é‡è¯„ä¼°dataset_20_items.jsonä¸­çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from IBench.pipeline.json_context_evaluator import JsonContextEvaluator


def batch_evaluate_dataset(dataset_path: str, output_dir: str):
    """
    æ‰¹é‡è¯„ä¼°æ•°æ®é›†

    Args:
        dataset_path: æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    test_cases = dataset['test_cases']
    print(f"âœ“ æ‰¾åˆ° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = JsonContextEvaluator()
    print("âœ“ è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ\n")

    # æ‰¹é‡è¯„ä¼°
    results = []
    summary = {
        "total_cases": len(test_cases),
        "evaluated_cases": 0,
        "failed_cases": 0,
        "total_score": 0,
        "case_details": []
    }

    print("="*60)
    print("å¼€å§‹æ‰¹é‡è¯„ä¼°")
    print("="*60 + "\n")

    for i, test_case in enumerate(test_cases, 1):
        case_key = test_case['key']
        case_desc = test_case.get('description', 'N/A')

        print(f"[{i}/{len(test_cases)}] æµ‹è¯•ç”¨ä¾‹ {case_key}: {case_desc}")

        try:
            # ä¿å­˜ä¸´æ—¶è¾“å…¥æ–‡ä»¶
            temp_input = os.path.join(output_dir, f"temp_input_{case_key}.json")
            with open(temp_input, 'w', encoding='utf-8') as f:
                json.dump(test_case, f, ensure_ascii=False, indent=2)

            # è¯„ä¼°
            temp_output = os.path.join(output_dir, f"output_{case_key}.json")
            result = evaluator.evaluate_from_json(temp_input, temp_output)

            # æå–è¯„ä¼°ç»“æœ
            evaluations = result['evaluations']
            total_score = sum(e['score'] for e in evaluations)
            triggered_rules = [e['rule'] for e in evaluations if e['triggered']]

            results.append(result)
            summary['evaluated_cases'] += 1
            summary['total_score'] += total_score

            print(f"  âœ“ å¾—åˆ†: {total_score}")
            print(f"  âœ“ è§¦å‘è§„åˆ™: {len(triggered_rules)} æ¡")
            print(f"  âœ“ ç”Ÿæˆå›å¤: {result['generated_response'][:50]}...")

            # è®°å½•è¯¦æƒ…
            summary['case_details'].append({
                "key": case_key,
                "description": case_desc,
                "score": total_score,
                "triggered_rules": triggered_rules,
                "total_rules": len(evaluations)
            })

        except Exception as e:
            print(f"  âœ— è¯„ä¼°å¤±è´¥: {e}")
            summary['failed_cases'] += 1
            summary['case_details'].append({
                "key": case_key,
                "description": case_desc,
                "error": str(e)
            })

        print()

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("="*60)
    print("è¯„ä¼°æ±‡æ€»")
    print("="*60)

    avg_score = summary['total_score'] / max(summary['evaluated_cases'], 1)
    summary['average_score'] = avg_score

    print(f"æ€»ç”¨ä¾‹æ•°: {summary['total_cases']}")
    print(f"æˆåŠŸè¯„ä¼°: {summary['evaluated_cases']}")
    print(f"å¤±è´¥æ•°é‡: {summary['failed_cases']}")
    print(f"æ€»å¾—åˆ†: {summary['total_score']}")
    print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")

    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = os.path.join(output_dir, "evaluation_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    all_results_path = os.path.join(output_dir, "all_results.json")
    with open(all_results_path, 'w', encoding='utf-8') as f:
        json.dump({
            "summary": summary,
            "results": results
        }, f, ensure_ascii=False, indent=2)

    print(f"âœ“ å®Œæ•´ç»“æœå·²ä¿å­˜: {all_results_path}")

    return summary


def main():
    """ä¸»å‡½æ•°"""
    # è·¯å¾„é…ç½®
    dataset_path = "examples/golden_history_dataset/dataset_20_items.json"
    output_dir = "data/output/eval_results"

    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_path):
        print(f"âœ— é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        print(f"  å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        return 1

    # æ‰¹é‡è¯„ä¼°
    try:
        summary = batch_evaluate_dataset(dataset_path, output_dir)
        print("\nâœ“ æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        return 0
    except Exception as e:
        print(f"\nâœ— æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
