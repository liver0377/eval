"""
æµ‹è¯•é»„é‡‘å†å²è¯„ä¼° Pipeline

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯ JsonContextEvaluator æ˜¯å¦æ­£å¸¸å·¥ä½œ
ä» golden_history_input.jsonl ä¸­è¯»å–ç¬¬ä¸€è¡Œä½œä¸ºæµ‹è¯•æ•°æ®
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from IBench.pipeline.json_context_evaluator import JsonContextEvaluator
from IBench.models.model_configs import Config


def test_golden_history_evaluation():
    """æµ‹è¯•é»„é‡‘å†å²è¯„ä¼° pipeline"""

    print("=" * 80)
    print("é»„é‡‘å†å²è¯„ä¼° Pipeline æµ‹è¯•")
    print("=" * 80)

    # 1. è¯»å–æµ‹è¯•æ•°æ®ï¼ˆgolden_history_input.jsonl çš„ç¬¬ä¸€è¡Œï¼‰
    input_file = project_root / "data" / "dataset" / "golden_history_input.jsonl"

    if not input_file.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶: {input_file}")
        print(f"   è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return False

    print(f"\nğŸ“‚ è¯»å–æµ‹è¯•æ•°æ®: {input_file}")

    with open(input_file, 'r', encoding='utf-8') as f:
        first_line = f.readline().strip()

    test_data = json.loads(first_line)
    print(f"âœ“ æˆåŠŸè¯»å–æµ‹è¯•æ•°æ®: key={test_data['key']}")
    print(f"  - æ¶ˆæ¯æ•°é‡: {len(test_data['messages'])}")
    print(f"  - è§„åˆ™æ•°é‡: {len(test_data['rule_list'])}")

    # 2. åˆå§‹åŒ–è¯„ä¼°å™¨
    print(f"\nğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")

    # è·å– API keyï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("\nâš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ° API key (DASHSCOPE_API_KEY æˆ– OPENAI_API_KEY)")
        print("   å°†ä½¿ç”¨ Mock æ¨¡å¼è¿›è¡Œæµ‹è¯•ï¼ˆä¸è°ƒç”¨çœŸå® APIï¼‰")
        print("\n   è¦ä½¿ç”¨çœŸå® APIï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
        print("   export DASHSCOPE_API_KEY='your-api-key'")
        print("   æˆ–è€…åœ¨ Windows ä¸Šï¼š")
        print("   set DASHSCOPE_API_KEY=your-api-key")

        # ä½¿ç”¨ Mock æ¨¡å¼
        try:
            # åˆ›å»º mock evaluator
            evaluator = create_mock_evaluator()
        except Exception as e:
            print(f"\nâŒ åˆ›å»º Mock evaluator å¤±è´¥: {e}")
            return False
    else:
        print(f"âœ“ æ‰¾åˆ° API key: {api_key[:10]}...")

        try:
            # åˆ›å»ºçœŸå®çš„ evaluator
            config = Config()
            evaluator = JsonContextEvaluator(
                config=config,
                api_key=api_key
            )
            print("âœ“ è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"\nâŒ åˆå§‹åŒ–è¯„ä¼°å™¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    # 3. è¿è¡Œè¯„ä¼°
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼°...")

    try:
        # åˆ›å»ºä¸´æ—¶è¾“å…¥æ–‡ä»¶
        temp_input = project_root / "temp_test_input.json"
        temp_output = project_root / "temp_test_output.json"

        with open(temp_input, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ ä¸´æ—¶è¾“å…¥æ–‡ä»¶: {temp_input}")

        # è¿è¡Œè¯„ä¼°
        result = evaluator.evaluate_from_json(
            input_json_path=str(temp_input),
            output_json_path=str(temp_output)
        )

        print(f"âœ“ è¯„ä¼°å®Œæˆ")

        # 4. æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  - ç”Ÿæˆçš„å›å¤: {result['generated_response'][:100]}...")

        total_score = sum(e['score'] for e in result['evaluations'])
        print(f"  - æ€»å¾—åˆ†: {total_score}")

        print(f"\n  è§„åˆ™è¯„ä¼°è¯¦æƒ…:")
        for eval_item in result['evaluations']:
            rule = eval_item['rule']
            triggered = eval_item['triggered']
            score = eval_item['score']
            reason = eval_item.get('reason', '')

            status = "âœ— è¿è§„" if triggered else "âœ“ é€šè¿‡"
            print(f"    {status} | {rule} | å¾—åˆ†: {score} | {reason[:50]}..." if len(reason) > 50 else f"    {status} | {rule} | å¾—åˆ†: {score} | {reason}")

        print(f"\nâœ… æµ‹è¯•æˆåŠŸï¼Pipeline è¿è¡Œæ­£å¸¸")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_input.exists():
            temp_input.unlink()
        if temp_output.exists():
            print(f"   è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {temp_output}")

        return True

    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def create_mock_evaluator():
    """åˆ›å»º Mock evaluatorï¼ˆç”¨äºæ²¡æœ‰ API key çš„æƒ…å†µï¼‰"""

    print("\nğŸ“ åˆ›å»º Mock evaluator...")

    # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æµ‹è¯•ï¼Œä¸ä¾èµ–çœŸå®æ¨¡å‹
    # ä¸»è¦éªŒè¯ pipeline çš„æµç¨‹æ˜¯å¦æ­£ç¡®

    class MockJsonContextEvaluator:
        """Mock evaluator for testing pipeline logic"""

        def __init__(self):
            self.dynamic_registry = None
            self.single_rule_registry = None
            self.stage_rule_registry = None

        def evaluate_from_json(self, input_json_path, output_json_path=None):
            """Mock evaluation"""

            # è¯»å–è¾“å…¥
            with open(input_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            key = data['key']
            messages = data['messages']
            rule_list = data['rule_list']

            # Mock ç”Ÿæˆçš„å›å¤
            generated_response = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›å¤ã€‚è¯·é—®æ‚¨è¿˜æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ"

            # Mock è¯„ä¼°ç»“æœ
            evaluations = []

            for rule_config in rule_list:
                if isinstance(rule_config, str):
                    rule_tag = rule_config
                else:
                    rule_tag = rule_config['rule']

                # ç®€å•çš„ mock é€»è¾‘
                triggered = rule_tag.startswith('single_turn')
                score = -1 if triggered else 1

                evaluations.append({
                    "rule": rule_tag,
                    "triggered": triggered,
                    "score": score,
                    "kwargs": {},
                    "reason": f"Mock è¯„ä¼°ç»“æœ for {rule_tag}"
                })

            result = {
                "key": key,
                "generated_response": generated_response,
                "evaluations": evaluations,
                "kwargs": [{} for _ in evaluations]
            }

            # ä¿å­˜è¾“å‡º
            if output_json_path:
                with open(output_json_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

            return result

    return MockJsonContextEvaluator()


def main():
    """ä¸»å‡½æ•°"""

    try:
        success = test_golden_history_evaluation()

        print("\n" + "=" * 80)

        if success:
            print("âœ… æµ‹è¯•å®Œæˆï¼šPipeline è¿è¡Œæ­£å¸¸")
            print("\nä¸‹ä¸€æ­¥ï¼š")
            print("1. é…ç½®çœŸå®çš„ API key è¿›è¡Œå®Œæ•´æµ‹è¯•")
            print("2. ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            print("3. æ‰¹é‡è¯„ä¼°ï¼šä½¿ç”¨ evaluate_batch_from_json()")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼šè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

        print("=" * 80)

        return 0 if success else 1

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
