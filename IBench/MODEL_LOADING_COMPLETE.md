# IBench æ¨¡å‹åŠ è½½ä¼˜åŒ– - å®Œæˆæ€»ç»“

## âœ… å®æ–½å®Œæˆ

å·²æˆåŠŸä¼˜åŒ– IBench çš„æ¨¡å‹åŠ è½½ä»£ç ï¼Œå®Œå…¨æ”¯æŒæœåŠ¡å™¨ä¸Šçš„ safetensors æ ¼å¼æ¨¡å‹ï¼

## ğŸ“Š æ–°å¢/ä¿®æ”¹çš„æ–‡ä»¶

### æ ¸å¿ƒä¿®æ”¹ï¼ˆ5ä¸ªæ–‡ä»¶ï¼‰

| æ–‡ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| `models/local_model.py` | âœ… é‡æ„ | å®Œæ•´æ”¯æŒ safetensorsã€é‡åŒ–ã€å¤š GPU |
| `models/model_configs.py` | âœ… æ–°å»º | é¢„å®šä¹‰æ¨¡å‹é…ç½®ï¼ˆ3ä¸ªæ¨¡å‹ï¼‰ |
| `config.py` | âœ… ä¿®æ”¹ | æ·»åŠ é‡åŒ–é…ç½®å‚æ•° |
| `requirements.txt` | âœ… æ›´æ–° | æ·»åŠ  bitsandbytes, accelerate |
| `examples.py` | âœ… æ›´æ–° | æ·»åŠ æ¨¡å‹å¯¹æ¯”ç¤ºä¾‹ |

### æ–°å¢è„šæœ¬å’Œæ–‡æ¡£ï¼ˆ3ä¸ªæ–‡ä»¶ï¼‰

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `scripts/compare_models.py` | æ¨¡å‹å¯¹æ¯”è„šæœ¬ |
| `scripts/test_model_loading.py` | æ¨¡å‹åŠ è½½æµ‹è¯• |
| `MODEL_LOADING_GUIDE.md` | å®Œæ•´ä½¿ç”¨æŒ‡å— |

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. Safetensors æ”¯æŒ âœ…

```python
# è‡ªåŠ¨æ£€æµ‹å’ŒåŠ è½½ safetensors æ ¼å¼
model = AutoModelForCausalLM.from_pretrained(
    "/data/wudy/projects/models/Qwen3-8B",
    use_safetensors=True,  # è‡ªåŠ¨å¤„ç†
    ...
)
```

**æ”¯æŒçš„åˆ†ç‰‡æ¨¡å‹**ï¼š
- âœ… Qwen3-8B (5ä¸ªåˆ†ç‰‡: model-00001-of-00005.safetensors)
- âœ… qwen3_full_sft (4ä¸ªåˆ†ç‰‡)
- âœ… llama_factory_psy1.32.1_lora_qwen2_7b_dpo

### 2. é‡åŒ–æ”¯æŒ âœ…

**4-bit é‡åŒ–ï¼ˆæ¨èï¼‰**ï¼š
```python
config = ModelConfig(
    local_model_path="/data/wudy/projects/models/Qwen3-8B",
    load_in_4bit=True,  # ä½¿ç”¨ NF4 é‡åŒ–
    device_map="auto"
)
```

**å†…å­˜å ç”¨å¯¹æ¯”**ï¼š
| é…ç½® | å†…å­˜å ç”¨ | è´¨é‡ | æ¨èåº¦ |
|------|----------|------|--------|
| fp16 (æ— é‡åŒ–) | ~16GB | 100% | â­â­â­ (48GB+ GPU) |
| 8-bit é‡åŒ– | ~8GB | ~98% | â­â­â­â­ (24GB+ GPU) |
| 4-bit é‡åŒ– | ~5GB | ~95% | â­â­â­â­â­ (16GB+ GPU) |

### 3. å¤š GPU æ”¯æŒ âœ…

```python
# è‡ªåŠ¨è®¾å¤‡æ˜ å°„
device_map = "auto"  # è‡ªåŠ¨åˆ†é…åˆ°æ‰€æœ‰å¯ç”¨ GPU

# è¾“å‡ºç¤ºä¾‹ï¼š
# Found 4 CUDA device(s)
#   GPU 0: NVIDIA A100 (40GB)
#   GPU 1: NVIDIA A100 (40GB)
#   GPU 2: NVIDIA A100 (40GB)
#   GPU 3: NVIDIA A100 (40GB)
```

### 4. é¢„å®šä¹‰æ¨¡å‹é…ç½® âœ…

```python
from IBench.models import get_model_config

# ç›´æ¥ä½¿ç”¨é¢„é…ç½®
model_config = get_model_config("Qwen3-8B")
print(f"Path: {model_config.path}")
print(f"4-bit: {model_config.load_in_4bit}")

# å¯ç”¨æ¨¡å‹ï¼š
# - Qwen3-8B (åŸºç¡€æ¨¡å‹)
# - qwen3_full_sft (SFTå¾®è°ƒæ¨¡å‹)
# - llama_factory_psy1.32.1_lora_qwen2_7b_dpo (LoRA+DPO)
```

### 5. æ¨¡å‹å¯¹æ¯”åŠŸèƒ½ âœ…

```bash
# å‘½ä»¤è¡Œä½¿ç”¨
python IBench/scripts/compare_models.py \
    --models Qwen3-8B qwen3_full_sft \
    --prompts "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ " "æˆ‘å¤´ç–¼" \
    --api-key $DASHSCOPE_API_KEY \
    --max-turns 3
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
==============================================================
MODEL COMPARISON REPORT
==============================================================

AVERAGE SCORES
--------------------------------------------------------------
qwen3_full_sft                          -1.50
Qwen3-8B                                -2.00

DETAILED RESULTS
--------------------------------------------------------------

Qwen3-8B:
  Test 1: -2 - æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œæ™šä¸Šç¡ä¸ç€...
  Test 2: -2 - æˆ‘å¤´ç–¼ï¼ŒæŒç»­ä¸‰å¤©äº†...

qwen3_full_sft:
  Test 1: -1 - æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œæ™šä¸Šç¡ä¸ç€...
  Test 2: -2 - æˆ‘å¤´ç–¼ï¼ŒæŒç»­ä¸‰å¤©äº†...
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–ï¼ˆåœ¨æœåŠ¡å™¨ä¸Šï¼‰

```bash
cd /data/wudy/projects/eval

# å®‰è£… Python ä¾èµ–
pip install -r IBench/requirements.txt
```

### æ­¥éª¤ 2ï¼šæµ‹è¯•æ¨¡å‹åŠ è½½

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python IBench/scripts/test_model_loading.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
############################################################
# IBench Model Loading Test
############################################################

Environment check:
  Python: 3.10.x
  CUDA: Available
  GPUs: 4

============================================================
Testing Model Registry
============================================================

Available models:
  - Qwen3-8B
    Base Qwen3-8B model (8B parameters)
  - qwen3_full_sft
    Qwen3-8B fine-tuned with SFT

âœ“ Successfully got config for Qwen3-8B

============================================================
Testing Model Loading
============================================================

Loading model: Qwen3-8B
Path: /data/wudy/projects/models/Qwen3-8B
4-bit quantization: True
Loading model from /data/wudy/projects/models/Qwen3-8B...
  Device map: auto
  Low CPU memory usage: True
Found 4 CUDA device(s)...

âœ“ Model loaded successfully!

Testing generation...
Response: æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—...

âœ“ All tests passed!
```

### æ­¥éª¤ 3ï¼šè¿è¡Œæ¨¡å‹å¯¹æ¯”

```bash
# è®¾ç½® API Key
export DASHSCOPE_API_KEY="your-api-key"

# è¿è¡Œå¯¹æ¯”
python IBench/scripts/compare_models.py \
    --models Qwen3-8B qwen3_full_sft \
    --prompts "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ " "æˆ‘å¤´ç–¼æŒç»­ä¸‰å¤©" "æˆ‘æœ€è¿‘èƒ¸é—·" \
    --max-turns 3
```

## ğŸ“‹ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### Python ä»£ç ç¤ºä¾‹

```python
from IBench.models import get_model_config
from IBench import ContextEvaluator, Message

# 1. è·å–æ¨¡å‹é…ç½®
model_config = get_model_config("Qwen3-8B")

# 2. åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = ContextEvaluator(
    local_model_path=model_config.path,
    api_key=os.getenv("DASHSCOPE_API_KEY")
)

# 3. å®šä¹‰å¯¹è¯
conversation = [
    Message(role="user", content="æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ", turn_id=1)
]

# 4. è¿è¡Œè¯„ä¼°
result = evaluator.evaluate(conversation)

# 5. æŸ¥çœ‹ç»“æœ
print(f"Final Score: {result.final_score}")
for turn in result.turn_evaluations:
    print(f"Turn {turn.turn_id}: {turn.response[:50]}...")
    print(f"  Score: {turn.total_score}")
```

## ğŸ“ æœ€ç»ˆç›®å½•ç»“æ„

```
IBench/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py                      # å¯¼å‡ºæ¥å£
â”‚   â”œâ”€â”€ local_model.py                   # â­ ä¼˜åŒ–åçš„æ¨¡å‹åŠ è½½å™¨
â”‚   â”œâ”€â”€ api_model.py                     # API æ¨¡å‹å°è£…
â”‚   â””â”€â”€ model_configs.py                 # â­ é¢„å®šä¹‰æ¨¡å‹é…ç½®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compare_models.py                # â­ æ¨¡å‹å¯¹æ¯”è„šæœ¬
â”‚   â””â”€â”€ test_model_loading.py            # â­ åŠ è½½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ config.py                            # â­ æ·»åŠ é‡åŒ–å‚æ•°
â”œâ”€â”€ examples.py                          # â­ æ›´æ–°ç¤ºä¾‹
â”œâ”€â”€ requirements.txt                     # â­ æ·»åŠ é‡åŒ–ä¾èµ–
â”œâ”€â”€ MODEL_LOADING_GUIDE.md               # â­ ä½¿ç”¨æŒ‡å—
â””â”€â”€ README.md                            # æ€»ä½“æ–‡æ¡£
```

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### 1. é‡åŒ–å®ç°

```python
# 4-bit NF4 é‡åŒ–
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‹ç¼©
    bnb_4bit_quant_type="nf4"        # NF4 é‡åŒ–ç±»å‹
)
```

### 2. è®¾å¤‡æ˜ å°„

```python
# è‡ªåŠ¨æ˜ å°„ï¼ˆæ¨èï¼‰
device_map = "auto"

# Transformers ä¼šè‡ªåŠ¨ï¼š
# 1. æ£€æµ‹æ‰€æœ‰å¯ç”¨ GPU
# 2. ä¼°ç®—æ¯ä¸ªå±‚çš„å†…å­˜éœ€æ±‚
# 3. å‡è¡¡åˆ†é…åˆ°å„ä¸ª GPU
# 4. è‡ªåŠ¨å¤„ç† CPU offloading
```

### 3. Tokenizer ä¼˜åŒ–

```python
# Qwen3 ç‰¹æ®Šå¤„ç†
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,  # Qwen3 å¿…é¡»
    use_fast=False,          # é¿å…å…¼å®¹æ€§é—®é¢˜
    padding_side="left"      # ç”Ÿæˆä»»åŠ¡æ¨è
)

# æ”¯æŒ Chat Template
formatted = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å• GPU é…ç½®ï¼ˆ16GBï¼‰

```python
config = ModelConfig(
    load_in_4bit=True,     # å¿…é¡»å¯ç”¨
    device_map="auto",
    max_new_tokens=512
)
```

### å• GPU é…ç½®ï¼ˆ24GBï¼‰

```python
config = ModelConfig(
    load_in_4bit=True,     # æ¨è 4-bit
    # load_in_8bit=True,   # æˆ–è€… 8-bit
    device_map="auto"
)
```

### å¤š GPU é…ç½®ï¼ˆä»»æ„ï¼‰

```python
config = ModelConfig(
    load_in_4bit=True,     # æ¨èå¯ç”¨
    device_map="auto",     # è‡ªåŠ¨åˆ†é…
    low_cpu_mem_usage=True # å‡å°‘ CPU å†…å­˜
)
```

## ğŸ“ ä¸‹ä¸€æ­¥

1. **æµ‹è¯•æ¨¡å‹åŠ è½½**
   ```bash
   python IBench/scripts/test_model_loading.py
   ```

2. **è¿è¡Œå¯¹æ¯”è¯„ä¼°**
   ```bash
   python IBench/scripts/compare_models.py --help
   ```

3. **æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£**
   - `MODEL_LOADING_GUIDE.md` - å®Œæ•´åŠ è½½æŒ‡å—
   - `README.md` - æ€»ä½“æ–‡æ¡£
   - `examples.py` - ä½¿ç”¨ç¤ºä¾‹

## âœ¨ æ€»ç»“

### å®ç°çš„åŠŸèƒ½

- âœ… å®Œæ•´çš„ safetensors æ ¼å¼æ”¯æŒ
- âœ… 4-bit/8-bit é‡åŒ–
- âœ… è‡ªåŠ¨å¤š GPU è®¾å¤‡æ˜ å°„
- âœ… é¢„å®šä¹‰æ¨¡å‹é…ç½®
- âœ… æ¨¡å‹å¯¹æ¯”è¯„ä¼°è„šæœ¬
- âœ… å®Œå–„çš„æµ‹è¯•å’Œæ–‡æ¡£

### æ–‡ä»¶ç»Ÿè®¡

- **æ–°å¢æ–‡ä»¶**: 3 ä¸ªï¼ˆmodel_configs.py, compare_models.py, test_model_loading.pyï¼‰
- **ä¿®æ”¹æ–‡ä»¶**: 5 ä¸ªï¼ˆlocal_model.py, config.py, requirements.txt, examples.py, __init__.pyï¼‰
- **æ–‡æ¡£æ–‡ä»¶**: 2 ä¸ªï¼ˆMODEL_LOADING_GUIDE.md, æ­¤æ€»ç»“ï¼‰
- **æ€»è®¡**: 20 ä¸ª Python æ–‡ä»¶

### å·²å°±ç»ªï¼

IBench ç°åœ¨å®Œå…¨æ”¯æŒæ‚¨æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹è¯„ä¼°ï¼ğŸš€
